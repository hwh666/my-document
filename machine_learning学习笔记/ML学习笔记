第一部分:监督学习(线性回归/逻辑回归/神经网络/SVM支持向量机)
分类器:包括线性回归分类器和逻辑回归分类器,他们不仅能解决线性拟合,也可以解决非线性拟合,不仅适用于单特征,也适用于多特征.
1.监督学习supervised learning,分为两种回归regression和classification分类问题,回归问题是输入和输出是连续函数,分类问题输入和输出是离散函数;
2.无监督学习unsupervised learning,也分为两种cluster和non-cluster;
3.梯度下降法中的迭代下降,第0个和第1个sitar是要同时变化的simultaneously;
4.一个变量的线性回归问题,linear regression with one variable;在机器学习里面,变量被称为特征,选取变量也就是选取特征;
5.特征变量要进行特征缩放和归一化,使得x在-0.5~0.5之间,用x减去平均值再除以最大值减最小值,这样可以加快收敛速度;
6.学习率,学习率太小会导致梯度下降慢,收敛慢,学习率太大会导致不收敛或震荡,因此要选择一个合适的学习率,吴恩达的方法是从0.001开始选择,0.001到0.003到0.01,这样慢慢往上加,找到一个合适的学习率;另外,画出迭代次数与代价函数的关系式可以帮助你直观了解学习率的影响;
7.对于特征的处理有两个步骤,一个是特征缩放feature scale,一个是均值归一化mean normalization;特征缩放是把特征值的取值范围变小,比如从0~1000变到0~1,这样可以加快梯度下降的速度;均值归一化则是采用公式(特征值-均值)/(特征值的最大值-特征值的最小值),这样得到的特征值是在-0.5~0.5;
8.多项式回归polynominal regression 就是用多项式去拟合数据集;多项式拟合就涉及到选用什么样的特征,以及选用哪种多项式;
9.正规方程法normal equation,正规方程法是区别与梯度下降法的另外一种计算最小值的方法,实际上就是求导数等于0,是一种解析方法,最终求得的是代价函数最小化的theta=(X转置*X)的逆*X的转置*y;正规方程法只能适用于线性回归,因为它不需要迭代,所有速度比较快,并且不需要考虑特征缩放和归一化,也不需要考虑调整学习率,但是它在特征数量达到一万左右,速度就会跟梯度下降法差不多,因为它要计算(X转置*X)的逆,计算量大概是特征数量的立方,所有到特征数量大于一万后,一般还是会选择梯度下降法;
10.在正规方程法中,有可能遇到(x转置*x)没有逆矩阵的问题,也就是(x转置*x)是奇异矩阵或退化矩阵,那么这个时候可能是两个原因导致的:一是所选的特征是相关的,比如说特征x1是房子的面积,特征x2是房子面积的平方,这时就应该删除相关的变量,只保留一个;另一个原因是特征数量n大于等于样本数量n,这时就应该删除一些特征;
11.线性回归法在分类问题上的效果是很不好的,所以有专门的算法,叫做逻辑回归logistic regression;
12.分类问题中的假设函数模板为 hθ(x)=g(θ转置*x),其中g(z)=1/(1+e的(-z次方)),这样就可以把假设函数hθ(x)的取值限制在(0,1),从物理意义上它代表了取Y=1的概率;其中的特征方程θ转置*x可以是线性的也可以是多项式表达的;决策边界decision boundary是区分Y=0和Y=1的特征变量的取值,比如说x1+x2<=3时Y=1,x1+x2>3时Y=0,这时就把x1+x2=3称为决策边界;根据训练样本得到假设函数,根据假设函数又可以得到决策边界,因此决策边界是假设函数的属性,与训练样本无直接关系;
13.对于logistic regression逻辑回归问题,不能用线性回归的代价函数,那样的代价函数会导致代价函数与theta的关系不是一个凸函数,就会出现很多局部极小值.因此逻辑回归有单独的代价函数,可见吴恩达课程week3的logistic regression model的cost function;逻辑回归相比与线性回归,一是假设函数不一样,二是代价函数也不一样;
14.对于多类分类,比如说n类,应该构建n个分类器,得到输入在每个类别的得分,然后选择得分最高的;
15.选用BFGS/L-BFGS/conjugate gradient等先进的优化算法,它们比梯度下降法好就好在收敛速度会大大快于梯度下降法,但这些算法都比较复杂;
16.正则化regularization,在回归问题中,可能会出现欠拟合underfitting,刚刚好just right,过拟合overfitting三种状态,过拟合会导致假设函数非常理想地符合所有训练数据,但实际是一条波动的曲线,它对新的数据的预测很有可能就是错的,这种情况就是泛化能力很弱.解决这个问题办法有,减少特征数量,或者正则化,其中正则化是常用的方法;正则化就是在代价函数中加入λ ∑(θj的平方)这一项,这一项作为惩罚项,会限制θj的值.λ的取值是有讲究的,如果λ过大,就是导致θj都很小,容易出现欠拟合,如果λ过小,对θj的限制就很小,无法消除过拟合;
17.在将正则化应用到求线性回归的两种方法中时,对于正规方法法中,添加了正则项后的代价函数,解析解解出来的theta解中,没有用正则化时,可能遇到(x转置*x)没有逆矩阵的问题,但正则化后,可以证明(x转置*x+λ⋅L(由于正则化而增加的部分))是一定可逆的;所以正则化也可以避免正规方法中的不可逆的问题;
18.为什么要使用神经网络算法:实际上第一部分的分类器已经非常强大,可以解决多特征非线性的问题,但是随着变量的增加,要实现好的拟合,特征数量会呈现几何级别的增加,比如说一个100个变量的分类问题,如果考虑二次项拟合,就会出现5000个二次项,也就是有5100个特征值,这个计算量就很大了,因此要采用别的方法;
19.神经网络的每个神经节点的激励函数也是g(z)=1/(1+e的(-z次方)),也就是与逻辑回归分类中的函数是一样的;
20.对于单个神经节点来说,它的激励函数就是逻辑回归函数,但是当从整个神经网络来看,中间的隐藏层对输入层的数据进行了逻辑回归函数处理,再作为输出层的输入,这些数据已经是输入数据的非线性化的数据,因此可以用来模拟非线性模型;
21.多层神经网络可以模拟更复杂的非线性函数,输入进入第一个隐藏层,计算出一些特征,然后进入下一个隐藏层,又计算出更复杂的一些特征,再进入下下一层,计算出更复杂一些的特征,这样直到最后的输出层,这样的神经网络就可以模拟一个复杂的非线性模型了;
22.神经网络中使用的假设函数就是逻辑回归中的假设函数,S型函数g(z)=1/(1+e的(-z次方));
23.不管是神经网络还是前面的逻辑回归方法,最终的参数的确定都要落脚到求解代价函数的最小值上来;
24.计算神经网络中的参数θ,也就是求解代价函数取最小值时的θ值,根据梯度下降法,也就要求解偏导数,在神经网络中这个偏导数的求导是通过正向传播forwardpropagation然后进行backpropagation反向传播来求解的;正向和反向传播就是用来求解偏导数,然后迭代更新θ的;
25.神经网络中的每一个神经单元都是一个冲击函数的计算,也就是S型函数g(z)=1/(1+e的(-z次方))的计算,其中z是这个神经单元的输入;
26.神经网络与线性回归或逻辑回归模型相比,区别就在于神经网络是多层的,每一层就相当于一个逻辑回归模型,而逻辑回归模型是单层的;也就是只是假设函数不一样,即对模型的假设不一样;
27.梯度检验,反向传播计算出来的θ的导数有可能不对,因此可以用梯度检验的方法来进行检验,先用反向传播计算出θ的导数,然后再用数值计算的方法(直接求解导数,用(f(θ+epsion)-f(θ-epsion))/2*epsion)来计算),比较这两种方法得到的值,如果基本相等的话,那么就说明反向传播计算没有错.为什么在梯度下降的时候不用数值计算的方法,是因为数值计算的方法太慢了,而反向传播求解导数的速度是比较快的.
28.神经网络中θ值的初始化:不能都直接初始化为0,这样会使每一层的所有特征都变得相等,也就是相当于都只有一个特征了,这叫做平衡,这样的网络就不能表现复杂的非线性关系,因此初始值的选择要打破平衡.一般来说,初始值的选择必须随机,使它们在一个范围(-epsion,+epsion),这个epsion是自己设定的;
29.神经网络结构的设计,首先隐藏层最好设计成一层,其次多层隐藏层的单元数最好都相等,再次,隐藏层的单元数一般越多越好,但是应该和输入层的特征数量对应,是它的两倍/三倍/四倍等;
30.反向传播算法:实际上不管是线性回归,逻辑回归,还是神经网络,最终都要使用梯度下降法来求解参数θ,而梯度下降法就必须要求偏导数,反向传播算法就是求解偏导数的一种方法,其他方法如数值法也可以,但是速度很慢.所以一般神经网络的参数求解就是用梯度下降法加上反向传播算法.可以看看14讲的书,在牛顿法里面用的是什么方法来求解偏导数?
31.神经网络的设计总体步骤:一是设计神经网络的框架;二是随机给定θ的初始值,一般来说都是设定的很小的数值;三是用一个for循环,对所有的训练集(Xi,Yi),依次进行前向传播和反向传播,前向传播可以求出每一层的每一个神经元的激励值a,反向传播可以求出每一层每一个神经元的偏差delta项,然后将所有循环的激励值和delta项累加可以得到偏导数;四是将反向传播算法求得的偏导数与数值计算求得的偏导数进行比较,确保反向传播计算的偏导数是正确的,然后将数值计算偏导数的程序关闭,因为它的计算很费时间;第五步就是利用梯度下降法,求解θ,直到收敛到极小值.
32.一般来说,神经网络产生的代价函数与θ的关系也并不是一个凸函数,因此有可能收敛到局部极小值,但是一般来说这个极小值也是能满足要求的;
33.如何选择好的假设函数或者判定一个假设函数好不好,这个叫做假设评价,一般的做法是把所有的数据集进行分类,一类分为训练集,另一类分为测试集.一般随机选择数据集中的七成作为训练集,剩下的三成作为测试集,先用训练集来得到假设函数,然后用测试集来计算代价函数,看假设函数在测试集上的表现如何(也就是代价函数的值大小);
34.如果是要在线性回归中,选择合适的多项式的阶数,那么就要把数据集分为三类:训练集/交叉验证集/测试集,对于每一个阶数的多项式,都用训练集来产生一个假设函数,然后把交叉验证集应用到这些假设函数中,求出对于的代价函数,选择其中代价函数最小的那个作为假设函数,然后将测试集应用到这个假设函数中来评价泛化能力;
35.对于一个已经设计好的神经网络或者回归算法,怎么来评价它是处于过拟合还是欠拟合,这就涉及两个概念,一个是高偏差(对应欠拟合)high bias,一个是高方差(对应过拟合)high variance.在欠拟合中,网络在训练集和交叉验证集的表现应该都不好,也就是代价函数都很大,并且差不多相等,所以叫做高偏差;而如果过拟合,训练集的代价函数应该很小,而在交叉验证集的代价函数应该很大,远远大于训练集的代价函数,所以这个叫做高方差;
36.不论是在回归算法还是神经网络中,都会涉及到几次项来做拟合,以及正则化参数应该选多少合适.这时候就要考虑选择好的算法的策略.比如说如何选择正则化参数,正则化是用来防止过拟合的,如果正则化参数选得过大,就会出现欠拟合,如果正则化参数选得过小,又会出现过拟合.这时的办法是,先用带正则项的代价函数,选择多个正则化参数,利用训练集,分别求出对应各个正则化参数的回归模型参数,然后用将这些模型分别应用到交叉验证集上,并且选择的交叉验证集的代价函数是不带正则项的,看哪个模型在交叉验证集上的代价函数最小,就选哪个模型(实际上也就是选择了它对应的正则化参数),最后再用测试集来测试,测试时用的代价函数也是不带正则项的.
37.学习曲线,它是训练集的代价函数和交叉验证集的代价函数与训练集样本数量的关系曲线,一般来说,随着样本数量的增加,训练集的代价函数是增加的,交叉验证集的代价函数是减小的;当处于欠拟合时,随着样本数量的增加,训练集的代价函数与交叉验证集的代价函数会逐渐接近,并且再增大样本数量,带来的交叉验证集的代价函数的减小是不明显的,因此这时增加样本数量并没有太大的作用;当处于过拟合时,随着样本的增加,训练集的代价函数一直增大,但始终比较小,但交叉验证集的代价函数虽然在减小,但一直比较大,也就是这两条曲线的间隔是比较大的,并且随着样本的增加,这两条曲线会逐渐靠拢,因此增加样本的数量对于过拟合是有用的.
38.如果算法拟合表现不好,那么针对是欠拟合(高偏差)和过拟合(高方差)分别有三种方法来改进.对于高偏差,可以增加额外的特征,增加多项式阶数,减小正则化参数;对于高方差,应该减少特征,增加样本数量,增大正则化参数.
39.对于神经网络来拟合函数,如果网络层数过少,就可能出现欠拟合,如果层数过多,就可能出现过拟合,但过拟合可以通过增加正则项来消除,所以多层网络的主要问题是计算量;
40.单纯的提高训练样本的数量,而不考虑算法是否过拟合或欠拟合,这种训练集数量的提升带来的算法效果的提升是很有限的;
41.正确的设计算法的步骤是:首先快速构建一个简单的网络,画出学习曲线,看它是大偏差还是大方差;然后在交叉验证集中测试,看它的表现如何.然后在结果中分析,比如分析识别错误的数据集,看他们有哪些特征,这些特征就可以加到原来的网络中去;还要采取定量的数值的误差分析方法,比如说识别的错误率,比如说应用在交叉验证集的错误率,看增加一个算法或者特征,带来了错误率怎样的变化,是变好了还是变坏了,这样就可以决定是否需要这个算法或者特征;
42.采用错误率来评价一个算法的好坏,在碰到偏斜类数据集(比如说数据集中y=1的数量远远小于y=0的数量)时会出现问题,它不能够很好的评价这个算法的好坏,这时候就要用到识准率perception和召回率recall,比如说y=1表示癌症,识准率是算法识别出的真实的癌症数(true positive)/(算法识别为癌症的数量(true positive + false positive));召回率是算法识别的真实的癌症数(ture positive)/(样本中的实际的癌症数量(true positive + false negative));识别率和召回率都高的算法就是好算法.查准率和召回率作为算法好坏的度量值.
43.查准率和召回率是两个相互矛盾的值,一般来说,比如在分类回归问题中,如果把y>=threshold作为1,y<threshold作为0,那么threshold的取值就会影响查准率和召回率.查准率高那么召回率就会下降,召回率高查准率就会下降,这取决于你的需要达到的目的.一般来说,要选择查准率P和召回率R都比较合适的值,可以用F值=2*(P*R)/(P+R)来作为评价指标,F值越大越好.
44.在设计一个好的机器学习算法时,如果想要用大量的数据训练来取得很好的效果,需要有两个前提条件:首先,不管是使用线性回归/逻辑回归等简单的机器学习算法,还是神经网络等复杂的机器学习算法,都要保证输入的特征向量X包含有足够多的特征,比如说要预测房子的价格就要包含面积/地段/新旧/家具情况等,这有一个很好的方法就是你可以考虑如果把这些信息提供给一个人类的专家,他是否能够根据这些信息准确估计房子的价格,如果可以,这些特征就是足够的;其次,对于线性回归/逻辑回归要考虑多项式的阶数,要用足够的阶数,而对于神经网络,要有足够的隐藏层及神经单元,这样用大量的数据训练时,才能有足够多的参数可以变化.前面这两条做到了就可以保证算法是低偏差的,也就是防止欠拟合.然后再用大量的数据来训练,当然加上正则项更好,即使没有正则项,通过大量的数据训练也可以避免过拟合的情况发生,也就是避免了高方差.这样低方差和低偏差的算法就是好的机器学习算法.
45.在面对分类问题时,SVM可以从逻辑回归算法中推出来,SVM与逻辑回归很相似,只是假设函数和代价函数不一样.SVM的假设函数是(θ的转置*X)大于0就y=1,其余的y=0;它的代价函数J也不一样,使用直线段来近似原来逻辑回归中的log函数,并且在y=1是需要(θ的转置*X)>1才能完全拟合,y=0时需要(θ的转置*X)<-1时才能完全拟合.
46.SVM的代价函数决定了它是一个大间距分类器(large margin classification),也就是说在两类数据的区分时,它的决策边界是离两类数据的间距都比较大的,这样的拟合曲线是很好的.
47.SVM在实际使用中的特征值一般会采用核函数的形式,核函数是选取一些种子点,然后对每个种子点利用高斯核来计算核函数样本与它的距离(实际上高斯核就是正态分布),一般在实际使用是会选取所有的训练集的点作为种子点;如果使用核函数,那么代价函数中也会使用核函数来替代;使用高斯核时,对于不同的特征,可能需要进行归一化或特征缩放;
48.SVM中有两个参数需要设置,一个是代价函数中的假设函数与样本的偏差部分的参数C,另一个是核函数的参数sigma.C=1/lamda,lamda是逻辑回归算法中的正则项,因此C越大就越可能过拟合,C越小就越可能欠拟合.sigma越大,核函数越平滑,就越可能欠拟合(高偏差),sigma越小,核函数越陡峭,就越可能过拟合(高方差);
49.好的SVM的库有libsvm和liblinear.首先你需要选择参数C,然后你需要选择核函数,一般通用核函数有两种,一种是高斯核函数,另一种是不用核函数,也称为线性核函数.当你的特征很多,但是样本很少时,为了防止过拟合,一般就用线性核函数;如果你的特征不多,但样本很多,就应该使用高斯核函数.
50.SVM的威力主要体现在可以使用核函数上面,通过核函数,可以拟合非常复杂的非线性曲线,这些曲线可能逻辑回归算法很能做到.如果不使用核函数,也就是使用线性核函数,它的效果其实和逻辑回归差不多;
51.如何选择使用逻辑回归还是SVM算法.如果特征比较多,但样本比较少,那么应该使用逻辑回归或者不带核函数的SVM;如果特征数量比较少,样本数量也是中等,比如说特征是1-10000个,样本是10-100000个,那么推荐使用带高斯核的SVM;如果是特征很少,但是样本数量特别大,比如样本数量达到百万级别,由于核函数计算比较慢,这个时候就应该选择逻辑回归或者不带核函数的SVM.当然,所有这些情况都可以用神经网络来实现,但是神经网络的训练比较慢.并且神经网络存在收敛到局部最小值的问题,但SVM本身是一个凸优化的问题,不存在收敛到局部最小值的问题.
52.监督学习中,逻辑回归/神经网络/SVM是常见的三种机器学习算法.


第二部分:无监督学习:K均值/PCA主成分分析/异常检测
53.无监督学习的一个典型例子是数据的聚类cluster;
54.K均值算法是无监督学习中的一种,它首先根据需要随机选取n个点,作为聚类中心(或者说簇的中心),然后将样本数据根据与n个点的距离,分配到n组中,然后对n个组求解平均值,再将新的n个聚类中心到n个平均值上,继续迭代直至n个聚类中心不动了.
55.k均值算法实际上也和监督学习算法一样,是有代价函数的,这个代价函数就是所有的点到它的簇的中心的距离的平方和,k均值算法的过程就是求这个代价函数的最小值.这个代价函数被称为失真代价函数;
56.K均值算法的初始聚类中心的选择,如果需要分为k类,那么就可以随机从样本点中选择k个点,作为聚类中心,然后以这k个聚类中心进行聚类,这时候得到的聚类结果可能是失真代价函数的局部最小值,所以要重复这个过程多次(比如说100次),然后从所有的聚类结果中,选择失真代价函数最小的.一般来说,这个方法在k值在1~10以内都是很好用的,当k很大时,往往执行一次聚类,可能就能得到不错的结果.但总之,执行多次聚类然后选择失真代价函数最小的值是可行的.
57.聚类数量的选择,也就是准备把数据分为多少类.这个问题没有明确的结论,一般有两种方法可以参考.一种是肘部法则elbow method,你可以尝试K从0,1,2等开始增大,然后进行聚类,画出各自对应的失真代价函数,失真代价函数应该是单调下降的,当从某个k值开始,失真代价函数下降开始明显变得缓慢,那么这个点就是肘点,就可以选择这个k值,但是实际情况中,肘点往往不明显.第二种方法是根据实际需要,我这个任务需要把数据分成多少组,那k就选多少.
58.第二种无监督学习算法叫做主成分分析PCA,主要用于维数约减diamension reduce,维数约减可以用来做数据压缩(比如三维变二维)以及算法提速,以及可视化(比如你有一个50维的数据,你需要降维到2维或者3维来显示);
59.PCA是寻找一个比数据低维的平面,然后把所有数据都投影到这个新的平面,使得投影距离最小;
60.PCA算法的过程是:如果要将一个n维的数据投影到k维,首先对特征进行缩放或者均值归一化,然后计算协方差矩阵sigma,然后对协方差矩阵进行svd分解得到U矩阵,然后取U矩阵的前k列就是k维平面的表示,然后用这个k维矩阵的转置乘以原来的n维度的样本x,就可以把n维度的样本全部转换成k维度的.这个算法是非常成熟的.同时,得到的缩减后的维度k,也可以通过相反的方法恢复原来的数据n,只是这个过程不能完全恢复原来的数据,但基本是近似的.
61.如何选择维度k,有一个评价指标叫平均平方投影误差(average squared projection error),是用原始数据x和映射值xapproox的距离的平方值的平均值,另一个叫做总变差,是原始数据的距离的平方值的平均值.评价一个PCA算法对原始数据的复现的好坏的指标使用平均平方投影误差除以总变差,小于某个值(比如说0.01),那么就可以说PCA保留了99%的差异性,也可以说是保留了99%的主成分.选择K的标准就是要求PCA算法能够保存足够量的主成分(比如说99%).步骤如下:首先求解协方差矩阵sigma,然后对sigma矩阵进行SVD分解得到U/S/V,其中S矩阵是一个对角阵,它的对角线上的数据,从第一个数据到第k个数据的累加和除以总对角线数据的累加和就等于平均平方投影误差除以总变差的值,所以只要看这个S矩阵,选择数量的对角线元素,使得PCA能保留足够的主成分,那么这个k值就确定了.
62.PCA算法用来加速算法的应用,它是将有很多特征的样本数据先进行PCA降维,找到x到z的对应关系,然后将z和标签y输入算法进行学习,得到拟合曲线.然后后续输入的所有样本都按此x到z的映射关系变化为z,再计算输出y.
63.PCA算法的三个主要应用,一是压缩数据,将高维数据降维到低维数据,节省硬盘和内存;第二个是用来可视化,将高维数据降维到2维或三维,便于显示;第三个是用来将输入数据降维,以使得学习算法加速;
64.异常检测问题,就是从一堆数据中找出数据的分布函数,然后对于新输入的数据,计算它的概率,看它的概率p是不是小于一个给定值e,从而判定它是不是异常数据.如果我们假定数据是成正态分布的,那么对数据求平均值就可以无偏估计正态分布的均值,求样本的方差也就是无偏估计正态分布的方差.
65.对于有多个特征的样本,比如说样本x有n个特征x1~xn,那么对每个特征都要求均值u_j和方差delta_j,都是根据样本均值和样本方差的方法来求.求出来之后,就得到了n个特征各自的概率分布p(xj),而总的x的概率分布p(x)是所有这些特征概率分布的乘积(这里是假设所有特征都是独立的),这样就求出来了x的分布函数.然后对于新的输入x,计算它的概率p(x),如果p(x)<e,就认为是异常,这里的e是自己设定的一个比较小的值.
66.使用异常检测问题算法的步骤,假定你刚开始是不知道使用哪些特征,也不知道e取多少合适.那么就可以按照下面的步骤开始:首先,你采集到的数据集中,应该是带标签的,也就是你知道它是正常还是异常,并且正常应该是占大多数的,那么就把数据集分为训练集(只包含正常的,大概占比60%),交叉验证集(包含正常的,占比20%,也包含不正常的,占比50%),测试集(包含正常的,占比20%,也包含异常的,占比50%),然后用训练集进行学习,得到分布函数p(x),然后在交叉验证集中使用这个分布函数去做预测,看预测的准确率,这里因为数据集是一个偏斜数据集,应该使用前面的识准率和召回率,或者是F1数来评价.那么可以用不同的特征值,看最终出来的F1的值的大小,也可以测试不同的e,比较最终的F1的大小,选择使得F1最大的特征和e最好.
67.异常检测和逻辑回归的区别,什么时候选择用哪一个?一般来说,如果正样本的数量(异常)很少,而负样本(正常)的数量很多的时候,就应该用异常检测算法,因为正样本太少,很难学习出什么规律来,这时候应该学习负样本的规律,也就是负样本的分布函数.而当正样本数量和负样本数量都很大时,就应该用逻辑回归算法,这样可以对异常进行具体的分类.
68.在异常检测中,因为每个特征都假定为正态分布,因此在选取特征后,对每个特征可以先画出直方图来,看看是不是像正态分布,如果不像,可以做一些处理,比如说求对数,求幂次方等,把它变化成别的样子,看起来像正态分布的样子,来作为特征.当然,这一步即使不做,异常检测的效果也不会差,但这样做了效果会更好.另外,特征选取是一个很复杂的事情,选哪些特征不选哪些特征,应该根据异常检测的效果来评价.比如你可以先快速搭建起算法,然后运行看看算法在交叉验证集上的表现如何,然后再来决定应该增加哪些特征,特征是可以根据现有的特征进行组合变化的,比如说用现有的两个特征做除法可以得到一个新的特征.
69.多元高斯分布,之前建立的高斯分布都是假设各个特征都是高斯分布并且独立的,而实际情况并不是这样,有时候各个特征是相关的,那么多元高斯分布就可以解决这个问题.多元高斯分布是把所有的特征综合起来,建立一个高斯分布函数p(x),而不是原来的各个特征的高斯分布的乘积,这时均值就变成了一个n维的向量(n是特征数量),标准差变成了一个n维的矩阵,叫协方差矩阵.均值和协方差的计算都有现成的公式.
70.基本高斯分布算法和多元高斯分布算法的选择:首先,多元高斯分布算法可以自动捕获特征间的相关性,而基本的高斯分布算法不可以,但是可以通过手动添加相关特征来弥补这个缺陷,比如说增加特征x1/x2;其次,多元高斯分布需要计算协方差矩阵的逆,当特征数量非常大时,这种计算是很消耗时间的,而基本的高斯分布算法没有这个问题;再次,多元高斯分布为了保证协方差矩阵可逆,要求样本数量m要大于特征数量n,实际使用中一般要求m>10*n,所以当样本数量比较少时,就不能使用多元高斯分布算法.
71.推荐系统:基于内容的推荐系统实际上就是一个变体的线性回归算法.是根据电影的特征来线性回归每个用户的喜好参数.
72.由于电影的特征评分很能获取,因此简单的线性回归系统并不好用,推荐系统中使用的好的算法是协同过滤算法,它分为两个部分.一个是假设知道了用户的喜好参数,来线性回归求解电影的特征参数;另一个是知道电影的特征参数,来线性回归求解用户的喜好参数.这就是涉及到一个先有鸡还是先有蛋的问题.一般的做法是,先随机给用户喜好参数赋值,然后线性回归求解电影的特征参数,然后在根据这个特征参数,线性回归求解用户喜好参数,然后再根据用户喜好参数,线性回归求解电影特征参数,如此反复迭代,最后会收敛到一个比较好的用户喜好参数和电影特征参数,这就叫做协同过滤算法.
73.协同过滤算法的反复轮流迭代实际上可以是去掉的,一种优化的协同过滤算法是,将电影的特征参数和用户的喜好参数一起作为变量写入到代价函数中,一起来通过线性回归进行优化,这样就可以同时求出电影的特征参数和用户的喜好参数.但这里要注意,这种方式的代价函数中是不需要电影特征的偏置项和用户喜好参数的偏置项的.
74.协同过滤算法可以用来给没有看过这部电影的人预测他会给这部电影打多少分,从而给他推荐.还可以根据电影特征来查找相似的电影.协同过滤算法预测电影打分可以用向量的形式来表现,因此该算法也被称为低秩矩阵分解.
75.在协同过滤算法中,比如对电影进行预测打分,可能会出现有一个用户没有对任何电影打分,那么如何预测它的喜好参数呢?如果直接使用协同过滤算法,它的喜好参数会都是0,预测的电影打分也都是0,这是不合理的.因此需要进行均值归一化,也就是把所有的样本数据都减去平均值,再用来进行协同过滤算法,最后在预测时再把平均值加上,这样就可以使得从没有打分的用户的预测电影得分等于平均值,这样是比较合理的.


第三部分:大数据集的使用
76.如果要使用大数据集,那么前面讲到的梯度下降法就会有非常大的计算量,因为每次计算偏导数都需要计算所有数据的假设函数值与标签的偏差值,因此之前的梯度下降法也被称为批量梯度下降法,而新的随机梯度下降法能解决这个问题.随机梯度下降法是每一次都只是对一个样本进行梯度下降,第一次是对第一个样本进行梯度下降,得到新的参数值,然后再对第二个样本进行梯度下降,得到更新后的梯度值,一直进行到所有样本结束.然后把整个这个过程再重复执行1~10遍.切记,在使用随机梯度下降法之前要打乱整个数据集.
77.小批量梯度下降法,与随机梯度下降法相比,每次计算一个较小值的样本量,比如2~100,而不是一个样本,这样的好处是每次计算一个较小的数量的样本求和,可以采用向量的方法来表示,从而可以采用并行计算.小批量梯度下降法是介于随机梯度下降法和批量下降法之间的,当选取的这个值为1时,那就是随机梯度下降法.
78.如何选择学习率:如果采用随机梯度下降法,可以再每次用一个样本进行参数更新前,先用上一次计算出来的参数求一下本次这个样本的代价函数,然后每1000次用这1000个样本的代价函数求平均值,也就是每1000个样本运行后求一次平均值,然后把这些平均值画出来,看看是否是逐渐下降的,从而判断是否收敛.如果是下降的,那么就是收敛的,如果不是下降的,比如所不收敛或者发散,就要考虑减小学习率,或者增加特征,或者改变其他东西.另外,随机梯度下降法中,如果你始终保持学习率不变,那么最终代价函数不会完全收敛到全局最小值,而是在全局最小值附近变化,如果你使得学习率随着迭代次数的增加而减小,比如说学习率等于一个常数除以迭代次数和另一个常数的和,那么代价函数会收敛得离全局最小值很近.但这种方法需要调试不同的常数一和常数二.
79.影响一个学习算法的因素:学习率,特征选取,数据量;
80.在线学习算法online learning,在线学习算法与随机梯度下降算法本质上是一样的,只是在线学习算法不需要一个固定的训练集,而是根据输入的每一个样本进行更新,然后把这个样本丢弃,它特别适合有大量不断输入的样本的情况.
81.映射约减算法:对于大数据集的批量梯度下降算法遇到的每次迭代的大量计算的问题,除了可以用随机梯度下降法来解决,还可以采用map reduce映射约减的方法来解决,它的思路是如果学习算法的代价函数和偏导数的求解是大量数据样本的函数的求和,那么就可以把样本分成多份,然后把每一份样本输入一台计算机中进行运算,最后再把多台计算机的结果相加,实际上就是并行计算的原理,这个方法也可以在带有多个核的单台计算机上实现.
82.把算法变成矩阵或矢量计算的重要性,当变成矢量或矩阵运行,就可以利用并行计算的方法来加速,很多开源的线性代数计算库都有并行计算的设计.


第四部分:实际使用
83.在OCR识别的例子中,使用滑动窗口的方法来进行字符的查找,分割和识别;
84.如果需要大量的数据集,可以采用人工合成的方法,也就是对现有的数据集进行变形等操作;
85.在改进学习算法之前,有两个重要的问题,首先是要画出学习曲线,确保现在的算法是低偏差或者高方差的,这样增大数据集才有意义,然后再仔细考虑要增大现在10倍的数据集,需要多久时间.
86.在机器学习的应用开发中,往往都有一个流水线,要找出流水线中哪个环节在整体系统的准确性上起主要作用,那么可以使用上限分析的方法.这个方法的大概流程是,先测试整个系统的准确性得分,然后从第一个模块开始,先去掉第一个模块,用一个准确的数据去输入到第二个模块开始后的系统,然后再测试系统的准确性得分,接着再去掉第二个模块,用一个准确的数据作为第三个模块开始后的系统的输入,再计算系统的准确性得分,这样一直到最后一个模块.比较这些准确性得分,看哪个模块部分提高得最多,就是目前系统性能影响最主要的模块.



<神经网络和深度学习>
1.结构化数据:例如表格等就叫结构化数据;非结构化数据:例如图片/语音等;

