方向投影：
1.概述

反向投影是一种记录给定图像中的像素点如何适应直方图模型像素分布的方式，简单来讲，反向投影就是首先计算某一特征的直方图模型，然后使用模型去寻找图像中存在的特征。反向投影在某一位置的值就是原图对应位置像素值在原图像中的总数目。
2.反向投影原理

原理采用OpenCV docs介绍！使用肤色直方图来解释反向投影的工作原理。假设我们已经获得一个肤色直方图(Hue-Staturation),旁边的直方图就是模型直方图(代表手掌的肤色色调)，可以通过掩码操作来抓取手掌所在区域的直方图：
这里写图片描述这里写图片描述
这里写图片描述这里写图片描述

我们需要做的就是使用模型直方图（代表手掌的皮肤色调）来检测测试图像中的皮肤区域。以下是检测步骤：
a.对测试图像中的每个像素 p(i,j),获取色调数据并找到该色调这里写图片描述在直方图中的bin位置
b.查询模型直方图中对应的bin-这里写图片描述并读取该bin的数值。
c.将此数值存储在新的图像中(BackProjection)。也可以先归一化模型直方图，这样测试图像的输出就可以在屏幕上显示了。
d.通过对测试中的图像中的每个像素采用以上步骤，可以得到如下的BackProjection结果图：
这里写图片描述
e.使用统计学的语言，BackProjection中存储的数值代表了测试图像中该像素属于皮肤区域的概率。以上图为例，亮的区域是皮肤区域的可能性更大，而暗的区域则表示更低的可能性。

这个过程可以笼统的说与计算图像直方图相反，由图像计算直方图的过程比较容易理解，就是统计图像中像素分布的概率，而反向投影正好反过来，是通过直方图来形成图像，其步骤有点类似于直方图均衡化，只不过直方图均衡化是将图像中的每个像素值由一个地方迁移到另外一个地方，而反向投影是直接去直方图中的值，如某种像素值在直方图中的值越大，在进行反向投影操作时其对应的像素值越大即月亮，反过来，如果某灰度值所占面积越小，其反向投影后像素值就会更小。

举个简单的例子来帮助理解这段话的意思。例如灰度图像的像素值如下如下
grayImage

0 1 2 3
4 5 6 7
8 9 10 11
8 9 14 15
对图像进行直方图统计（bin指定的区间为[0,3)，[4,7)，[8,11)，[12,16)）如下所示：
Histogram=

4 4 6 2
也就是说在[0,3)这个区间的像素值有4个，其它含义相同
根据上述的直方图进行反向投影，得到反向投影图像像素值如下：
Back_Projection=

4 4 4 4
4 4 4 4
6 6 6 6
6 6 2 2
例如位置(0,0)上的像素值为0，对应的bin为[0,3)，所以反向直方图在该位置上的值这个bin的值4，而在位置(3,3)上的像素为15，其在直方图中的统计为2，故其反向投影图像中的像素为2
3.OpenCV提供API

calcBackProjection()函数共有三种形式，根据传入参数的不同选择不同的调用，为重载函数

void cv::calcBackProject    (   const Mat *     images,
        int     nimages,
        const int *     channels,
        InputArray      hist,
        OutputArray     backProject,
        const float **      ranges,
        double      scale = 1,
        bool    uniform = true 
    )

    1
    2
    3
    4
    5
    6
    7
    8
    9

参数解释：
const Mat* images:输入图像，图像深度必须位CV_8U,CV_16U或CV_32F中的一种，尺寸相同，每一幅图像都可以有任意的通道数
int nimages:输入图像的数量
const int* channels:用于计算反向投影的通道列表，通道数必须与直方图维度相匹配，第一个数组的通道是从0到image[0].channels()-1,第二个数组通道从图像image[0].channels()到image[0].channels()+image[1].channels()-1计数
InputArray hist:输入的直方图，直方图的bin可以是密集(dense)或稀疏(sparse)
OutputArray backProject:目标反向投影输出图像，是一个单通道图像，与原图像有相同的尺寸和深度
const float ranges**:直方图中每个维度bin的取值范围
double scale=1:可选输出反向投影的比例因子
bool uniform=true:直方图是否均匀分布(uniform)的标识符，有默认值true

另外两种定义形式如下：

void cv::calcBackProject    (   const Mat *     images,
        int     nimages,
        const int *     channels,
        const SparseMat &   hist,
        OutputArray     backProject,
        const float **      ranges,
        double      scale = 1,
        bool    uniform = true 
    )

    1
    2
    3
    4
    5
    6
    7
    8
    9

void cv::calcBackProject    (   InputArrayOfArrays      images,
        const std::vector< int > &      channels,
        InputArray      hist,
        OutputArray     dst,
        const std::vector< float > &    ranges,
        double      scale 
    )   

    1
    2
    3
    4
    5
    6
    7

4.示例代码

#include <iostream>
#include <opencv2/core.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/imgproc.hpp>

using namespace std;
using namespace cv;

//定义全局变量
Mat srcImage, hsvImage,hueImage;
const int hueBinMaxValue = 180;
int hueBinValue=25;

//声明回调函数
void Hist_and_Backprojection(int, void*);

int main()
{
    srcImage=imread("Back_Projection_Theory2.jpg");

    //判断图像是否加载成功
    if(srcImage.empty())
    {
        cout << "图像加载失败" << endl;
        return -1;
    }
    else
        cout << "图像加载成功..." << endl << endl;

    //将图像转化为HSV图像
    cvtColor(srcImage, hsvImage, CV_BGR2HSV);

    //只使用图像的H参数
    hueImage.create(hsvImage.size(), hsvImage.depth());
    int ch[]={0,0};
    mixChannels(&hsvImage, 1, &hueImage, 1, ch, 1);

    //轨迹条参数设置
    char trackBarName[20];
    sprintf(trackBarName,"Hue bin:%d",hueBinMaxValue);
    namedWindow("SourceImage",WINDOW_AUTOSIZE);

    //创建轨迹条并调用回调函数
    createTrackbar(trackBarName, "SourceImage", &hueBinValue, hueBinMaxValue, Hist_and_Backprojection);
    Hist_and_Backprojection(hueBinValue, 0);

    imshow("SourceImage", srcImage);

    waitKey(0);

    return 0;
}

void Hist_and_Backprojection(int, void*)
{
    MatND hist;
    int histsize=MAX(hueBinValue, 2);
    float hue_range[]={0,180};
    const float* ranges={hue_range};

    //计算图像直方图并归一化处理
    calcHist(&hueImage, 1, 0, Mat(), hist, 1, &histsize, &ranges, true, false);
    normalize(hist, hist, 0, 255, NORM_MINMAX, -1, Mat());

    //获取反向投影
    MatND backProjection;
    calcBackProject(&hueImage, 1, 0, hist, backProjection, &ranges, 1, true);

    //输出反向投影
    imshow("BackProjection", backProjection);

    //绘制图像直方图
    int w=400;
    int h=400;
    int bin_w = cvRound((double)w/histsize);
    Mat histImage = Mat::zeros(w, h, CV_8UC3);
    for(int i=0; i < hueBinValue; i++)
    {
        rectangle(histImage, Point(i*bin_w, h), Point((i+1)*bin_w, h-cvRound(hist.at<float>(i)*h/255.0)), Scalar(0,0,255), -1);
    }
    imshow("HistImage", histImage);
}

2.基于块的反向投影
1.反向投影的作用是什么？
    反向投影用于在输入图像（通常较大）中查找特定图像（通常较小或者仅1个像素，以下将其称为模板图像）最匹配的点或者区域，也就是定位模板图像出现在输入图像的位置。
2.反向投影如何查找（工作）？
    查找的方式就是不断的在输入图像中切割跟模板图像大小一致的图像块，并用直方图对比的方式与模板图像进行比较。

假设我们有一张100x100的输入图像，有一张10x10的模板图像，查找的过程是这样的：
（1）从输入图像的左上角(0,0)开始，切割一块(0,0)至(10,10)的临时图像；
（2）生成临时图像的直方图；
（3）用临时图像的直方图和模板图像的直方图对比，对比结果记为c；
（4）直方图对比结果c，就是结果图像(0,0)处的像素值；
（5）切割输入图像从(0,1)至(10,11)的临时图像，对比直方图，并记录到结果图像；
（6）重复（1）～（5）步直到输入图像的右下角。

3.反向投影的结果是什么？
    反向投影的结果包含了：以每个输入图像像素点为起点的直方图对比结果。可以把它看成是一个二维的浮点型数组，二维矩阵，或者单通道的浮点型图像。
4.特殊情况怎么样？
    如果输入图像和模板图像一样大，那么反向投影相当于直方图对比。如果输入图像比模板图像还小，直接罢工～～。
5.使用时有什么要注意的地方？
    需要注意的地方比较多，我们对照反向投影函数来说：
    (void cvCalcBackProjectPatch
        IplImage** image,   
        CvArr* dst,         
        CvSize patch_size,  
        CvHistogram* hist,  
        int method,         
        float factor        
    );
    还有最需要注意的地方：这个函数的执行效率非常的低，在使用之前尤其需要注意图像的大小，直方图的维数，对比方式。如果说对比单个直方图对现在的电脑来说是清风拂面，那么反向投影是狂风海啸。对于1010x1010的RGB输入图像，10x10的模板图像，需要生成1百万次3维直方图，对比1百万次3维直方图。

2.图像背景模型的codebook算法：
codebook算法从原理上讲仍然属于背景差分。他的背景建模有聚类的思想，他是用一个码本(codebook)cb来描述一个像素p，cb中包含着若干码元(code element)ce，这些ce就是该像素点p的一个聚类，码本算法背景建模的过程就是要构建像素的一个个聚类，即码本。码本算法的背景建模过程是一个统计的过程，即它会在建模阶段将某点出现的可能的像素值进行统计，根据其出现的某种特征属性，设定阀值进行判断，符合条件的像素值即可作为背景像素。
例如，要进行建模的帧数为5帧（为举例方便，一般建模应该在一秒以上，30帧左右，当然不是建模所用的帧数越多越好），在这5帧中相同的某点p(x,y)，像素取值依次是pi =  {135,140,200,145,135}（此处假设所建模的对象是单通道的灰度图），那么在建模过程中，当第一帧像素值135到来时，由于p点对应像素值的码本没有码元，那么久新建一个码元，并为该码元赋值，设定code_element的各项属性值，第二帧到来后，140像素值会和已有的第一个码元进行匹配比较，在码元中有一对属性值learn_high和learn_low，这对属性值规定了该码元能“融合”的像素值范围，当满足条件135-learn_low < 140 < 135+learn_high时，即可将140像素融合，若不满足则为140像素新建一个码元，此处假设learn_high和learn_low均为10，那么可融合140，融合后更新该码元的一些数据，例如出现频数f要加1，stale最久未更新时间，他是最后更新时间t_last_update和当前时间t的差值，同理之后的第3、4、5帧按此过程，被融合或者新建码元，会发现只有像素200没有被融合，自己新建了一个码元。当匹配完后，就要进入下一个阶段——剔除噪声码元，剔除的依据是设定合理的阀值，若不满足条件则认为是噪声，剔除依据的属性可以使频数f或者最久未更新时间stale，或者是二者的组合条件，具体使用什么样的剔除方法，要根据自己想要的结果进行选择，此处设立条件为f > t/2，即如果一个码元的更新频数如果小于一半建模帧数就认为它是噪声，将其删除，即认为该码元出现的次数太少，不认为他是长时间保持稳定不变的背景像素。那么，经过剔除的码元即是可以合理描述背景的码元。
其他位置的像素点也是依照上述的方法，建立起合理的背景码元模型。
1，CodeBook算法流程介绍

　　CodeBook算法的基本思想是得到每个像素的时间序列模型。这种模型能很好地处理时间起伏，缺点是需要消耗大量的内存。CodeBook算法为当前图像的每一个像素建立一个CodeBook(CB)结构,每个CodeBook结构又由多个CodeWord(CW)组成。
　　CB和CW的形式如下：
　　CB={CW1,CW2,…CWn,t}
　　CW={lHigh,lLow,max,min,t_last,stale}
　　其中n为一个CB中所包含的CW的数目，当n太小时，退化为简单背景，当n较大时可以对复杂背景进行建模;t为CB更新的次数。CW是一个6元组，其中IHigh和ILow作为更新时的学习上下界，max和min记录当前像素的最大值和最小值。上次更新的时间t_last和陈旧时间stale(记录该CW多久未被访问)用来删除很少使用的CodeWord。
假设当前训练图像I中某一像素为I(x,y)，该像素的CB的更新算法如下，另外记背景阈值的增长判定阈值为Bounds：
　　(1) CB的访问次数加1；
　　(2) 遍历CB中的每个CW，如果存在一个CW中的IHigh，ILow满足ILow≤I(x,y)≤ IHigh，则转(4)；
　　(3) 创建一个新的码字CWnew加入到CB中, CWnew的max与min都赋值为I(x,y)， IHigh <- I(x,y) + Bounds，ILow <- I(x,y) – Bounds，并且转(6)；
　　(4) 更新该码字的t_last，若当前像素值I(x,y)大于该码字的max，则max <- I(x,y)，若 I(x,y)小于该码字的min，则min <- I(x,y)；
　　(5) 更新该码字的学习上下界，以增加背景模型对于复杂背景的适应能力，具体做法是： 若IHigh < I(x,y) + Bounds，则IHigh 增长1，若ILow > I(x,y) – Bounds，则ILow 减少1；
　　(6) 更新CB中每个CW的stale。
　　使用已建立好的CB进行运动目标检测的方法很简单，记判断前景的范围上下界为minMod和maxMod，对于当前待检测图像上的某一像素I(x,y)，遍历它对应像素背景模型CB中的每一个码字CW，若存在一个CW，使得I(x,y) < max + maxMod并且I(x,y) > min – minMod，则I(x,y)被判断为背景，否则被判断为前景。
　　在实际使用CodeBook进行运动检测时，除了要隔一定的时间对CB进行更新的同时，需要对CB进行一个时间滤波，目的是去除很少被访问到的CW，其方法是访问每个CW的stale，若stale大于一个阈值（通常设置为总更新次数的一半），移除该CW。
　　综上所述，CodeBook算法检测运动目标的流程如下：
　　(1) 选择一帧到多帧使用更新算法建立CodeBook背景模型；
　　(2) 按上面所述方法检测前景（运动目标）；
　　(3) 间隔一定时间使用更新算法更新CodeBook模型，并对CodeBook进行时间滤波；
　　(4) 若检测继续，转(2)，否则结束。
3.分水岭算法
分水岭算法主要用于图像分段，通常是把一副彩色图像灰度化，然后再求梯度图，最后在梯度图的基础上进行分水岭算法，求得分段图像的边缘线。

        下面左边的灰度图，可以描述为右边的地形图，地形的高度是由灰度图的灰度值决定，灰度为0对应地形图的地面，灰度值最大的像素对应地形图的最高点。

ima1 (2)ima2

我们可以自己编程实现灰度图的地形图显示，工程FirstOpenCV6就实现了简单的这个功能，比如上边的灰度图，显示为：

image

对灰度图的地形学解释，我们我们考虑三类点：

1. 局部最小值点，该点对应一个盆地的最低点，当我们在盆地里滴一滴水的时候，由于重力作用，水最终会汇聚到该点。注意：可能存在一个最小值面，该平面内的都是最小值点。

2. 盆地的其它位置点，该位置滴的水滴会汇聚到局部最小点。

3. 盆地的边缘点，是该盆地和其它盆地交接点，在该点滴一滴水，会等概率的流向任何一个盆地。

image

       假设我们在盆地的最小值点，打一个洞，然后往盆地里面注水，并阻止两个盆地的水汇集，我们会在两个盆地的水汇集的时刻，在交接的边缘线上(也即分水岭线)，建一个坝，来阻止两个盆地的水汇集成一片水域。这样图像就被分成2个像素集，一个是注水盆地像素集，一个是分水岭线像素集。

      下面的gif图很好的演示了分水岭算法的效果：

lpe1 (1)ima3 (1)

     

     在真实图像中，由于噪声点或者其它干扰因素的存在，使用分水岭算法常常存在过度分割的现象，这是因为很多很小的局部极值点的存在，比如下面的图像，这样的分割效果是毫无用处的。

ima7ima7b

      为了解决过度分割的问题，可以使用基于标记(mark)图像的分水岭算法，就是通过先验知识，来指导分水岭算法，以便获得更好的图像分段效果。通常的mark图像，都是在某个区域定义了一些灰度层级，在这个区域的洪水淹没过程中，水平面都是从定义的高度开始的，这样可以避免一些很小的噪声极值区域的分割。

      下面的gif图很好的演示了基于mark的分水岭算法过程：

ima4lpe2ima5

      上面的过度分段图像，我们通过指定mark区域，可以得到很好的分段效果：

 

4.OPENCV特征点检测算法：
识别算法概述：

SIFT/SURF基于灰度图，

一、首先建立图像金字塔，形成三维的图像空间，通过Hessian矩阵获取每一层的局部极大值，然后进行在极值点周围26个点进行NMS，从而得到粗略的特征点，再使用二次插值法得到精确特征点所在的层（尺度），即完成了尺度不变。

 

二、在特征点选取一个与尺度相应的邻域，求出主方向，其中SIFT采用在一个正方形邻域内统计所有点的梯度方向，找到占80%以上的方向作为主方向；而SURF则选择圆形邻域，并且使用活动扇形的方法求出特征点主方向，以主方向对齐即完成旋转不变。

 

三、以主方向为轴可以在每个特征点建立坐标，SIFT在特征点选择一块大小与尺度相应的方形区域，分成16块，统计每一块沿着八个方向占的比例，于是特征点形成了128维特征向量，对图像进行归一化则完成强度不变；而SURF分成64块，统计每一块的dx，dy，|dx|，|dy|的累积和，同样形成128维向量，再进行归一化则完成了对比度不变与强度不变。

 

haar特征也是基于灰度图，

首先通过大量的具有比较明显的haar特征（矩形）的物体图像用模式识别的方法训练出分类器，分类器是个级联的，每级都以大概相同的识别率保留进入下一级的具有物体特征的候选物体，而每一级的子分类器则由许多haar特征构成（由积分图像计算得到，并保存下位置），有水平的、竖直的、倾斜的，并且每个特征带一个阈值和两个分支值，每级子分类器带一个总的阈值。识别物体的时候，同样计算积分图像为后面计算haar特征做准备，然后采用与训练的时候有物体的窗口同样大小的窗口遍历整幅图像，以后逐渐放大窗口，同样做遍历搜索物体；每当窗口移动到一个位置，即计算该窗口内的haar特征，加权后与分类器中haar特征的阈值比较从而选择左或者右分支值，累加一个级的分支值与相应级的阈值比较，大于该阈值才可以通过进入下一轮筛选。当通过分类器所以级的时候说明这个物体以大概率被识别。

 

广义hough变换同样基于灰度图，

使用轮廓作为特征，融合了梯度信息，以投票的方式识别物体，在本blog的另一篇文章中有详细讨论，这里不再赘述。

特点异同对比及其适用场合：

 

三种算法都只是基于强度（灰度）信息，都是特征方法，但SIFT/SURF的特征是一种具有强烈方向性及亮度性的特征，这使得它适用于刚性形变，稍有透视形变的场合；

haar特征识别方法带有一点人工智能的意味，对于像人脸这种有明显的、稳定结构的haar特征的物体最适用，只要结构相对固定即使发生扭曲等非线性形变依然可识别；

广义hough变换完全是精确的匹配，可得到物体的位置方向等参数信息。前两种方法基本都是通过先获取局部特征然后再逐个匹配，只是局部特征的计算方法不同，SIFT/SURF比较复杂也相对稳定，haar方法比较简单，偏向一种统计的方法形成特征，这也使其具有一定的模糊弹性；

广义hough变换则是一种全局的特征——轮廓梯度，但也可以看做整个轮廓的每一个点的位置和梯度都是特征，每个点都对识别有贡献，用直观的投票，看票数多少去确定是否识别出物体。

 
SIFT/SURF算法的深入剖析——谈SIFT的精妙与不足

SURF算法是SIFT算法的加速版，OpenCV的SURF算法在适中的条件下完成两幅图像中物体的匹配基本实现了实时处理，其快速的基础实际上只有一个——积分图像haar求导，对于它们其他方面的不同可以参考本blog的另外一篇关于SIFT的文章。

 

    不论科研还是应用上都希望可以和人类的视觉一样通过程序自动找出两幅图像里面相同的景物，并且建立它们之间的对应，前几年才被提出的SIFT（尺度不变特征）算法提供了一种解决方法，通过这个算法可以使得满足一定条件下两幅图像中相同景物的某些点（后面提到的关键点）可以匹配起来，为什么不是每一点都匹配呢？下面的论述将会提到。

 

     SIFT算法实现物体识别主要有三大工序，

1、提取关键点；

2、对关键点附加详细的信息（局部特征）也就是所谓的描述器；

3、通过两方特征点（附带上特征向量的关键点）的两两比较找出相互匹配的若干对特征点，也就建立了景物间的对应关系。

 

      日常的应用中，多数情况是给出一幅包含物体的参考图像，然后在另外一幅同样含有该物体的图像中实现它们的匹配。两幅图像中的物体一般只是旋转和缩放的关系，加上图像的亮度及对比度的不同，这些就是最常见的情形。基于这些条件下要实现物体之间的匹配，SIFT算法的先驱及其发明者想到只要找到多于三对物体间的匹配点就可以通过射影几何的理论建立它们的一一对应。首先在形状上物体既有旋转又有缩小放大的变化，如何找到这样的对应点呢？于是他们的想法是首先找到图像中的一些“稳定点”，这些点是一些十分突出的点不会因光照条件的改变而消失，比如角点、边缘点、暗区域的亮点以及亮区域的暗点，既然两幅图像中有相同的景物，那么使用某种方法分别提取各自的稳定点，这些点之间会有相互对应的匹配点，正是基于这样合理的假设，SIFT算法的基础是稳定点。

 

SIFT算法找稳定点的方法是找灰度图的局部最值，由于数字图像是离散的，想求导和求最值这些操作都是使用滤波器，而滤波器是有尺寸大小的，使用同一尺寸的滤波器对两幅包含有不同尺寸的同一物体的图像求局部最值将有可能出现一方求得最值而另一方却没有的情况，但是容易知道假如物体的尺寸都一致的话它们的局部最值将会相同。SIFT的精妙之处在于采用图像金字塔的方法解决这一问题，我们可以把两幅图像想象成是连续的，分别以它们作为底面作四棱锥，就像金字塔，那么每一个截面与原图像相似，那么两个金字塔中必然会有包含大小一致的物体的无穷个截面，但应用只能是离散的，所以我们只能构造有限层，层数越多当然越好，但处理时间会相应增加，层数太少不行，因为向下采样的截面中可能找不到尺寸大小一致的两个物体的图像。有了图像金字塔就可以对每一层求出局部最值，但是这样的稳定点数目将会十分可观，所以需要使用某种方法抑制去除一部分点，但又使得同一尺度下的稳定点得以保存。有了稳定点之后如何去让程序明白它们之间是物体的同一位置？研究者想到以该点为中心挖出一小块区域，然后找出区域内的某些特征，让这些特征附件在稳定点上，SIFT的又一个精妙之处在于稳定点附加上特征向量之后就像一个根系发达的树根一样牢牢的抓住它的“土地”，使之成为更稳固的特征点，但是问题又来了，遇到旋转的情况怎么办？发明者的解决方法是找一个“主方向”然后以它看齐，就可以知道两个物体的旋转夹角了。下面就讨论一下SIFT算法的缺陷。

 

      SIFT/SURT采用henssian矩阵获取图像局部最值还是十分稳定的，但是在求主方向阶段太过于依赖局部区域像素的梯度方向，有可能使得找到的主方向不准确，后面的特征向量提取以及匹配都严重依赖于主方向，即使不大偏差角度也可以造成后面特征匹配的放大误差，从而匹配不成功；另外图像金字塔的层取得不足够紧密也会使得尺度有误差，后面的特征向量提取同样依赖相应的尺度，发明者在这个问题上的折中解决方法是取适量的层然后进行插值。SIFT是一种只利用到灰度性质的算法，忽略了色彩信息，后面又出现了几种据说比SIFT更稳定的描述器其中一些利用到了色彩信息，让我们拭目以待。

 

      最后要提一下，我们知道同样的景物在不同的照片中可能出现不同的形状、大小、角度、亮度，甚至扭曲；计算机视觉的知识表明通过光学镜头获取的图像，对于平面形状的两个物体它们之间可以建立射影对应，对于像人脸这种曲面物体在不同角度距离不同相机参数下获取的两幅图像，它们之间不是一个线性对应关系，就是说我们即使获得两张图像中的脸上若干匹配好的点对，还是无法从中推导出其他点的对应。
算法实现：
  vector<KeyPoint> key1,key2;
  Ptr<SIFT> sift;
  sift = SIFT::create();
//  Ptr<SURF> surf;
//  surf=SURF::create(800);
  BFMatcher matcher;
  Mat c,d;
  vector<DMatch> matches;
  sift->detectAndCompute(img,Mat(),key1,c);
  sift->detectAndCompute(img2,Mat(),key2,d);
  drawKeypoints(img,key1,img,Scalar(255,0,0));
  drawKeypoints(img2,key2,img2,Scalar(255,0,0));
  matcher.match(c,d,matches);
  sort(matches.begin(),matches.end());
  vector<DMatch> good_matches;
  int ptsPairs = min(500,(int)(matches.size()*0.15));
  cout<<ptsPairs<<endl;
  for(int i=0;i<ptsPairs;i++)
  {
    good_matches.push_back(matches[i]);
  }
  Mat outimg;
  drawMatches(img,key1,img2,key2,matches,outimg,Scalar(255,0,0),Scalar(0,0,0),vector<char>());


